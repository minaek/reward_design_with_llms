# Introduction
This codebase is adapted from a [PyTorch](http://pytorch.org/) implementation of research paper [Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://arxiv.org/abs/1706.05125) developed by [Facebook AI Research](http://research.fb.com/category/facebook-ai-research-fair).

The code trains neural networks to hold negotiations in coarse dialogue acts, and allows reinforcement learning training and rollout-based planning.

# Setup
All code is developed with Python 3.8.13. We recommend creating a conda environment based on the `env.yml` file:
```conda env create -f env.yml```

# Prompts Used in the Paper
The exact prompts used in the paper can be generated by `base_prompts.py`. You can print out prompts by running the following:
```python base_prompts.py --style [stubborn, versatile, competitive, pushover]```

# Training Models
## Training a SL Model
We train an SL model first in order to seed our RL agents. Run the following script to train SL with default parameters:
```python sl.py```

## Training a Baseline SL Reward Model
To train the baseline reward model using supervised learning, run the following:
```python sl_baseline.py --style [stubborn, versatile, competitive, pushover]```

## Training an RL model 
You can choose to train an RL model with {GPT3, ground truth reward, SL reward, GPT2} as the reward function. To run RL with the default parameters, specify a reward model and a style:
```python reinforce.py --style [stubborn, versatile, competitive, pushover] --model [gpt3, rl, sl_baseline, gpt2]```
Logs containing the negotiations the agent has during training will be saved under `logs`

## Evaluating your RL model
We evaluate our trained RL model against Bob, a model trained with SL. Run the following to evaluate your model:
```python selfplay.py --style [stubborn, versatile, competitive, stubborn] --model [gpt3, rl, sl_baseline, gpt2]```
Logs containing the negotiations the agent has during evaluation will be saved under `eval_logs`

## Evaluating the Reward Model's Labeling Accuracy
Run the following after training an RL agent:
```python reward_model_accuracy.py --style [stubborn, versatile, competitive, stubborn] --model[gpt3, sl_baseline, gpt2]```
